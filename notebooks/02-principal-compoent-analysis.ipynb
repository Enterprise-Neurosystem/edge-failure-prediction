{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ada6da1-a75c-4428-92d5-8ea7c5ddd009",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "pca is a mathematical operation that finds the most important features, or combination of features, contributing to your end value. In our case, we search for sensors that indicate, or indicate against, a future machine failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802878a-8521-4610-b39f-2e8040883429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5befd-47a1-40fa-8932-b2c06889a585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if the a directory exists, if not create it\n",
    "outdir = './scratch'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a827a-b790-469d-a86a-e44b637eed99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read our cleaned csv into a pandas dataframe\n",
    "df = pd.read_csv(outdir +'/clean-df.csv')\n",
    "\n",
    "#The index got reset, lets put the timestamp back\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8e7b8-2a9a-4a63-b108-d76fb156b78f",
   "metadata": {},
   "source": [
    "### We can take a look at some of the sensors to better understand the real-time changes and break downs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dda9d-5068-4879-8dd1-1f6444e8c8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_subset = df[['sensor_03', 'sensor_04', 'sensor_21', 'machine_status']]\n",
    "\n",
    "#Matplotlib allows us to define we want to use subplots for each columns (not all the lines on one plot) and the overall figure size.\n",
    "df_subset.plot( kind='line',figsize=(12,6),subplots=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50861e-a0c2-4eb5-a107-ca2d873134e5",
   "metadata": {},
   "source": [
    "### Let's keep track of when those failures above happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06950886-e3cf-4389-acbb-62259dde1620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "failure_times = df[df['machine_status'] == 1]\n",
    "print(\"Failure times:\")\n",
    "failure_times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22e3d7b1-1b2f-4bff-9fac-770c616da000",
   "metadata": {},
   "source": [
    "### For our model, we'll need to designate which parts of the data should train the model and which should be used for testing. The validation split is a way of testing and checking against over-fitting, without using the actual test data.\n",
    "\n",
    "### A common practice is to just split your data randomly, or by a defined percent. However, we want to make sure each split has at least one failure and recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabc901-ea52-4d12-a456-990551aa6079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the failures and specifically the first two\n",
    "timestamps = list(failure_times.index)\n",
    "first_failure = timestamps[0]\n",
    "second_failure = timestamps[1]\n",
    "\n",
    "# create a time buffer for our sensors to recover\n",
    "two_hours = pd.Timedelta(seconds=60 * 120)\n",
    "\n",
    "# split our large dataframe\n",
    "# validation will include all the data up to the first failure (plus to hours)\n",
    "# testing will include the following data up until the second failure (plus to hours)\n",
    "# training data includes everything else\n",
    "df_val = df.loc[:(first_failure + two_hours), :]\n",
    "df_test = df.loc[(first_failure + two_hours):(second_failure + two_hours), :]\n",
    "df_train = df.loc[(second_failure + two_hours):, :]\n",
    "\n",
    "# get a better look at the results\n",
    "print('Validation size:')\n",
    "print(str(df_val.shape)+ '  ' + str(round(len(df_val) / len(df)*100,2)) + '%')\n",
    "print('Test size:')\n",
    "print(str(df_test.shape)+ '  ' + str(round(len(df_test) / len(df)*100,2)) + '%')\n",
    "print('Train size:')\n",
    "print(str(df_train.shape)+ '  ' + str(round(len(df_train) / len(df)*100,2)) + '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a719271-b785-4360-ab93-bde94bec7e45",
   "metadata": {},
   "source": [
    "### For better training accuracy we should try and avoid big discrepancies between the sensor averages. Normalizing all the columns does the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4013978-c2c4-4b51-927f-0fddfdd377a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.matrix(scaled_train).transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c713ba5-9667-43ba-88a2-cfcbaef26978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display original ranges\n",
    "ranges = pd.DataFrame([{'min':i,'max':j} for i, j in zip(df_train.min(), df_train.max())])\n",
    "col_names = df_train.max().keys()\n",
    "ranges.index = col_names\n",
    "display(ranges.transpose())\n",
    "\n",
    "sensor_names = df_train.columns\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#finds the variables needed to transform our data to the [0,1] range we defined above\n",
    "scaler = min_max_scaler.fit(df_train[sensor_names])\n",
    "\n",
    "# transforms each dataframe\n",
    "scaled_val = scaler.transform(df_val[sensor_names])\n",
    "scaled_test = scaler.transform(df_test[sensor_names])\n",
    "scaled_train = scaler.transform(df_train[sensor_names])\n",
    "\n",
    "column_arrays = np.array(scaled_train).T\n",
    "ranges = pd.DataFrame([{'min':k.min(),'max':k.max()} for k in column_arrays])\n",
    "ranges.index = col_names\n",
    "display(ranges.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b325690-debe-45cf-a28d-0d3c2f6f2bd3",
   "metadata": {},
   "source": [
    "### We're reading to run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a6212-df97-4342-9b6c-bbd7f3714f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=scaled_train.shape[1]).fit(scaled_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95680f6-435d-4a80-852a-25ca4a194e3a",
   "metadata": {},
   "source": [
    "### Starting with a single feature, and each time a new feature is added, we will check to see how much variance within machine_status can be explained by the changes in the subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76e14a-633c-4417-8421-0d2cec12dfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, df_train.shape[1]-1, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, df_train.shape[1]+1, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(25, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d0900-6207-4f8d-85d2-e9c322b8d2d4",
   "metadata": {},
   "source": [
    "### 95% accuracy is good enough for us\n",
    "\n",
    "### Now, lastly, let's get those actual values we are correlating to the variance. Each Principal component contributes a different amount to the end goal, so we will include the ones the contribute most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93f3da-a442-4649-8374-6811badb1c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pc_to_include = 12\n",
    "\n",
    "def transform_df_pca(pca, data, scaled_data):\n",
    "    \"\"\"Reduce the number of features in the training data by the parameter num_features_to_include.\n",
    "\n",
    "    :param pca: The PCA for the training data\n",
    "    :type: PCA\n",
    "    :param df_data: data dataframe\n",
    "    :type: Pandas DataFrame\n",
    "    :param scaled_data: Array of scaled data\n",
    "    :type: ndarray\n",
    "    :param num_pc_to_include: Number of features to include.  Currently, this number is determined\n",
    "    by the num of PC's that were chosen by the PCA to reach 95% ( sum of explained_variance_ratio)\n",
    "    :type: int\n",
    "    \n",
    "    :return: DataFrame with data that has been scaled and whose dimensions have been reduced.  This DataFrame has\n",
    "    the same index as the param df_data\n",
    "    :type: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    #Transform our raw data with the fitted pca numbers\n",
    "    data_transformed = pca.transform(scaled_data)  # ndarray\n",
    "    df_transformed = pd.DataFrame(data_transformed)\n",
    "    \n",
    "    #Rename the columns\n",
    "    pcs = ['pc' + str(i + 1) for i in range(pca.n_components_)]\n",
    "    df_transformed.columns = pcs\n",
    "\n",
    "    #Provide our new numbers the same timestamps and machine_status as the original dataset\n",
    "    df_transformed.index = data.index\n",
    "    df_transformed = df_transformed[pcs[:num_pc_to_include]]\n",
    "    df_transformed['machine_status'] = data['machine_status'].values\n",
    "    \n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "#Transform each dataset\n",
    "df_train_pca = transform_df_pca(pca, df_train, scaled_train)\n",
    "df_test_pca = transform_df_pca(pca, df_test, scaled_test)\n",
    "df_val_pca = transform_df_pca(pca, df_val, scaled_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e1a720-5f5d-4a33-b4d8-9c59454850ee",
   "metadata": {},
   "source": [
    "#### We can now think of our rows as vectors currently contributing to directional changes in machine_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8b961-8c91-45ba-96be-4d98b788effc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
