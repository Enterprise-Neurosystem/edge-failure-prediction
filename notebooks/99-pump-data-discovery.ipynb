{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install numpy --upgrade\n",
    "#!{sys.executable} -m pip install pandas --upgrade\n",
    "#!{sys.executable} -m pip install matplotlib --upgrade\n",
    "#!{sys.executable} -m pip install sklearn --upgrade\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import psycopg2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "os.chdir('../PumpDataDiscovery')\n",
    "cwd = os.getcwd()\n",
    "print(\"Current working directory: {0}\".format(cwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_as_df():\n",
    "    conn = psycopg2.connect(\n",
    "    database=\"postgres\", user='ad_postgres', password='AWSw0rksh0p', \n",
    "    host='db-anomalydetect-postgres.chanowujpkf4.us-east-1.rds.amazonaws.com')\n",
    "    GET_ALL_ROWS = 'Select * from waterpump order by timestamp'\n",
    "    try:\n",
    "        with conn:\n",
    "            df = pd.read_sql_query(GET_ALL_ROWS, conn)\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "    except (Exception, psycopg2.DatabaseError) as err:\n",
    "        print(err)\n",
    "    else:\n",
    "        return df\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_list(df):\n",
    "    \"\"\"Get list of sums of nulls.\n",
    "    Get list by columns showing how many nulls are in each column\n",
    "    :param df: DataFrame\n",
    "    :type: Pandas DataFrame\n",
    "    :return: List of sums of nulls\n",
    "    :type: List\n",
    "    \"\"\"\n",
    "    nulls_series = df.isnull().sum()\n",
    "    return nulls_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_bad_cols(df, col_list):\n",
    "    \"\"\"Drop unusable columns inplace\n",
    "\n",
    "    :param df: DataFrame\n",
    "    :type: Pandas DataFrame\n",
    "    :param col_list: List of unusable column names\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    df.drop(col_list, axis=1, errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_status_to_numeric(df):\n",
    "    \"\"\"Make 'machine_status\" column numeric\n",
    "    Numeric values are 0: 'NORMAL';, 1: 'BROKEN', 0.5: 'RECOVERING'\n",
    "    :param df: DataFrame\n",
    "    :type: Pandas DataFrame\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    status_values = [(df['machine_status'] == 'NORMAL'), (df['machine_status'] == 'BROKEN'),\n",
    "                     (df['machine_status'] == 'RECOVERING')]\n",
    "    numeric_status_values = [0, 1, 0.5]\n",
    "\n",
    "    df['machine_status'] = np.select(status_values, numeric_status_values, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failure_times(df):\n",
    "    \"\"\"Get DataFrame of rows where 'machine_status' has a value of 1\n",
    "\n",
    "    :param df: DataFrame\n",
    "    :return: Failure times\n",
    "    :type: DatetimeIndex\n",
    "    \"\"\"\n",
    "    return df[df['machine_status'] == 1].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specific_cols(df):\n",
    "    df[['sensor_03', 'sensor_04', 'sensor_21', 'machine_status']].plot( kind='line', \n",
    "                                             figsize=(10,4),subplots=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failure_times(df):\n",
    "    \"\"\"Get DataFrame of rows where 'machine_status' has a value of 1\n",
    "\n",
    "    :param df: DataFrame\n",
    "    :return: Failure times\n",
    "    :type: DatetimeIndex\n",
    "    \"\"\"\n",
    "    return df[df['machine_status'] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(df, failure_times):\n",
    "    \"\"\"Slice data by failure times\n",
    "    If there are 7 failure times, then produce 7 slices as DataFrames\n",
    "\n",
    "    :param df: All Data\n",
    "    :type: Pandas DataFrame\n",
    "    :param failure_times: DatetimeIndex\n",
    "    :return: DataFrames for training, validation and testing\n",
    "    :type: tuple of DataFrames\n",
    "    \"\"\"\n",
    "    df_val = df.loc[:(failure_times[0] + pd.Timedelta(seconds=60 * 120)), :]\n",
    "    df_test = df.loc[(failure_times[0] + pd.Timedelta(seconds=60 * 120)):(\n",
    "            failure_times[1] + pd.Timedelta(seconds=60 * 120)), :]\n",
    "    df_train = df.loc[failure_times[1] + pd.Timedelta(seconds=60 * 120):, :]\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_mean(df):\n",
    "    \"\"\"\n",
    "    First replace all empty cells with NaN\n",
    "    Then replace NaN columnwise with mean of each column inplace\n",
    "    :param df: DataFrame\n",
    "    :type: Pandas DataFrame\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    df_no_blank = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df.fillna(value=df_no_blank[cols].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(training_df):\n",
    "    \"\"\"Get a scaler for training data\n",
    "    Apply MinManScaler to the training data using the range (0, 1)\n",
    "    :param training_df:\n",
    "    :type: Pandas DataFrame\n",
    "    :param sensor_names: List of sensor names(feature names)\n",
    "    :type: list\n",
    "    :return: scaler for the training data\n",
    "    :type: MinMaxScaler\n",
    "    \"\"\"\n",
    "    sensor_names = training_df.columns[:-2]\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = min_max_scaler.fit(training_df[sensor_names])\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe(scaler, data_df):\n",
    "    \"\"\"Transform given dataframe using given scaler as applied to the given columns\n",
    "\n",
    "    :param scaler: The scaler that has been fit to the dataframe\n",
    "    :type: MinMaxScaler\n",
    "    :param data_df: Dataframe to be scaled\n",
    "    :type: Pandas DataFrame\n",
    "    :param sensor_names: List of sensor names\n",
    "    :type: List of string\n",
    "    :return: array of scaled data\n",
    "    :type: ndarray\n",
    "    \"\"\"\n",
    "    sensor_names = data_df.columns[:-2]\n",
    "    scaled_data = scaler.transform(data_df[sensor_names])\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA(train_scaled_data, num_components=None):\n",
    "    \"\"\"Get PCA that has been fit to the scaled training data\n",
    "    PCA is the Principal Component Analysis.  Training data features are each ranked by their maximum variance from\n",
    "    all the other features.\n",
    "    :param train_scaled_data: scaled training data\n",
    "    :type: ndarray\n",
    "    :param num_components: Number of features in the training data\n",
    "    :return: Fit PCA\n",
    "    :type: PCA\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=num_components).fit(train_scaled_data)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(pca, n_features):\n",
    "    #pca = PCA().fit(data_rescaled)\n",
    "\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    xi = np.arange(1, n_features, step=1)\n",
    "    y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.xticks(np.arange(1, n_features, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The number of components needed to explain variance')\n",
    "\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.text(25, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df_by_pca(pca, df_data, scaled_data, num_pc_to_include):\n",
    "    \"\"\"Reduce the number of features in the training data by the parameter num_features_to_include.\n",
    "\n",
    "    :param pca: The PCA for the training data\n",
    "    :type: PCA\n",
    "    :param df_data: data dataframe\n",
    "    :type: Pandas DataFrame\n",
    "    :param scaled_data: Array of scaled data\n",
    "    :type: ndarray\n",
    "    :param num_pc_to_include: Number of features to include.  Currently, this number is determined\n",
    "    by the num of PC's that were chosen by the PCA to reach 95% ( sum of explained_variance_ratio)\n",
    "    :type: int\n",
    "    \n",
    "    :return: DataFrame with data that has been scaled and whose dimensions have been reduced.  This DataFrame has\n",
    "    the same index as the param df_data\n",
    "    :type: Pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    data_transformed = pca.transform(scaled_data)  # ndarray\n",
    "    df_transformed = pd.DataFrame(data_transformed)\n",
    "    pcs = ['pc' + str(i + 1) for i in range(pca.n_components_)]\n",
    "    df_transformed.columns = pcs\n",
    "\n",
    "    df_transformed.index = df_data.index\n",
    "    df_smaller = df_transformed[pcs[:num_pc_to_include]]\n",
    "    df_smaller['machine_status'] = df_data['machine_status'].values\n",
    "\n",
    "    return df_smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data \n",
    "Read the data into a dataframe, set the dataframe's index to the 'timestamp' column and parse the dates so that they become datetime objects.  Get a count of null values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('sensor.csv', index_col='timestamp', parse_dates=True)\n",
    "df = get_all_as_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null count for each column\n",
    "Make a list of columns that have too many nulls and remove those columns from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_series = get_null_list(df)\n",
    "print(nulls_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_col_list = ['Unnamed: 0', 'sensor_00', 'sensor_15', 'sensor_50', 'sensor_51']\n",
    "drop_bad_cols(df, bad_col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 'machine_status' values with numerics\n",
    "Convert the textual machine_status values, 'BROKEN', 'NORMAL', 'RECOVERING' to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "machine_status_to_numeric(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some sensor data\n",
    "Plot three sensor values and compare the plots to the plot of the machine_status column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_specific_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace blank and NaN values with mean for each column\n",
    "Replace all blank values in the dataframe with NaN and then replace all NaN values with the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nan_with_mean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get failure times\n",
    "Get all failure times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_times = get_failure_times(df)\n",
    "print(\"Failure times: {}\".format(failure_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by failure times\n",
    "Separate data into training, validation and test dataframes.  The slicing will be done as follows:\n",
    "    \n",
    "    Validation data will consist of all rows from the start of the data file to two minutes after the first failure time\n",
    "    Test data will consist of all rows from two minutes after the first failure to two minutes after the second failure time\n",
    "    Training data will consist of all rows from two minutes after the second failure to the end of the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = separate_data(df, failure_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MinMaxScaler for training data\n",
    "Fit a MinMaxScaler to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_scaler = get_scaler(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform all data using MinMaxScaler that was fit to training data\n",
    "Scale transform the training, validation, and test data using the training scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = scale_dataframe(training_scaler, df_train)\n",
    "scaled_val = scale_dataframe(training_scaler, df_val)\n",
    "scaled_test = scale_dataframe(training_scaler, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PCA that is fit on the training data\n",
    "Fit the scaled training data with a PCA analysis.  Plot the cumulative sum of the variance to determine how many PC's \n",
    "are needed to reach 95% of the threshold.\n",
    "\n",
    "The Principal Component Analysis (PCA) makes linear combinations ( Called Principal Components) of all the features and then ranks those combinations by their per cent contribution to the total variance. We then see how many Principal\n",
    "Components we need to account for a threshold of 95% of the total variance.  We will see by the graph generated that we will need 12 PC's to account for 95%.\n",
    "\n",
    "We will then use 12 PC's instead of 47 sensors to do our model training, testing and prediction.  So by using the PCA transformation, we reduce our need for 47 features to only 12 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = get_PCA(scaled_train)\n",
    "plot_pca(pca, len(df_train.columns) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform all the data using the fitted PCA\n",
    "Transform the scaled training, validation and test data using the fitted PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pc_to_include = 12\n",
    "df_train_pca = transform_df_by_pca(pca, df_train, scaled_train, num_pc_to_include)\n",
    "df_val_pca = transform_df_by_pca(pca, df_val, scaled_val, num_pc_to_include)\n",
    "df_test_pca = transform_df_by_pca(pca, df_test, scaled_test, num_pc_to_include)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pca.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
